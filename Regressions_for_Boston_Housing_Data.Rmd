---
title: "Linear Regression and LASSO on Boston Housing Data"
author: "Anumeha Dwivedi"
date: "01 Feb 2018"
output: rmarkdown::github_document
---

###Loading libraries
```{r Loading Libraries, message=FALSE, warning=FALSE}
library(MASS)
library(GGally)
library(corrplot)
library(glmnet)
library(dplyr)
library(leaps)
```

###Exploratory Data Analysis 
```{r Data exploration, echo=TRUE, message=FALSE, warning=FALSE, results = 'hide'}
data(Boston)
dim(Boston)
cormat=cor(Boston)
corrplot(cormat, type = "upper", method="number")
Boston$chas<-as.factor(Boston$chas)
```

###Plotting the variables
```{r EDA, message=FALSE, warning=FALSE}
plot(medv~lstat, data=Boston)
plot(medv~rm, data=Boston)
counts <- table(Boston$chas)
barplot(counts, main="", xlab="chas")
```

###Splitting into test and train sets
```{r Test Train Split, message=FALSE, warning=FALSE}
set.seed(123)
index <- sample(nrow(Boston),nrow(Boston)*0.80) #80-20 split
Boston.train <- Boston[index,]
Boston.test <- Boston[-index,]
```


###Building linear regression model
```{r message=FALSE, warning=FALSE}
model0<- lm(medv~lstat, data = Boston.train)
model1<- lm(medv~., data=Boston.train)
model2<- lm(medv~. -indus -age, data=Boston.train)
AIC(model0)
BIC(model0)
AIC(model1)
BIC(model1)
AIC(model2)
BIC(model2)
anova(model2, model1)
summary(model2)
```

As the p-value obtained for ANOVA is around 0.6, we can say that indus and age do not contribute as much towards predicting house prices

###Best subset selection
```{r message=FALSE, warning=FALSE}
model.subset<- regsubsets(medv~.,data=Boston.train, nbest=1, nvmax = 13)
subset_fit=summary(model.subset)
subset_fit
```

###Stepwise selection
```{r message=FALSE, warning=FALSE}
null.model<-lm(medv~1, data=Boston)
full.model<-lm(medv~., data=Boston.train)
result<-step(null.model, scope=list(lower=null.model, upper=full.model), k = 2, direction="forward")
result$anova
result<-step(full.model, scope=list(lower=null.model, upper=full.model), k = 2, direction="backward")
result$anova
result<-step(null.model, scope=list(lower=null.model, upper=full.model), k = 2, direction="both")
result$anova
```

###LASSO regression
```{r LASSO Regression, message=FALSE, warning=FALSE}
Boston$chas<-as.numeric(Boston$chas)
#Standardize covariates before fitting LASSO
Boston.X.std<- scale(select(Boston,-medv))
X.train<- as.matrix(Boston.X.std)[index,]
X.test<-  as.matrix(Boston.X.std)[-index,]
Y.train<- Boston[index, "medv"]
Y.test<- Boston[-index, "medv"]

lasso.fit<- glmnet(x=X.train, y=Y.train, family = "gaussian", alpha = 1)
plot(lasso.fit, xvar = "lambda", label=TRUE)
```


###Cross-validation in LASSO regression
```{r Cross Validation in LASSO, message=FALSE, warning=FALSE}
cv.lasso<- cv.glmnet(x=X.train, y=Y.train, family = "gaussian", alpha = 1, nfolds = 10)
plot(cv.lasso)
cv.lasso$lambda.min
cv.lasso$lambda.1se
pred.lasso.train<- predict(lasso.fit, newx = X.train, s=cv.lasso$lambda.min)
pred.lasso.min<- predict(lasso.fit, newx = X.test, s=cv.lasso$lambda.min)
pred.lasso.1se<- predict(lasso.fit, newx = X.test, s=cv.lasso$lambda.1se)
#Lasso MSE
mean((Y.train-pred.lasso.train)^2)
#MSPE
mean((Y.test-pred.lasso.min)^2)
mean((Y.test-pred.lasso.1se)^2)
```
